"""
Comprehensive Model Evaluation Module
Includes metrics, visualization, and statistical tests
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, confusion_matrix, classification_report,
                           roc_auc_score, roc_curve, auc, cohen_kappa_score)
from sklearn.model_selection import cross_val_score, StratifiedKFold
import scipy.stats as stats
from typing import Dict, Tuple, List
import json

class GNSSEvaluator:
    """Comprehensive evaluation of classification models"""
    
    def __init__(self, class_names=None):
        """
        Args:
            class_names: List of class names
        """
        self.class_names = class_names or ['LOS', 'MULTIPATH', 'NLOS']
        self.results = {}
        
    def comprehensive_evaluation(self, y_true: np.ndarray, y_pred: np.ndarray, 
                                y_proba: np.ndarray = None, 
                                model_name: str = "Model") -> Dict:
        """
        Perform comprehensive evaluation
        
        Returns:
            Dictionary with all metrics
        """
        results = {}
        
        # Basic metrics
        results['accuracy'] = accuracy_score(y_true, y_pred)
        results['precision_macro'] = precision_score(y_true, y_pred, average='macro', zero_division=0)
        results['recall_macro'] = recall_score(y_true, y_pred, average='macro', zero_division=0)
        results['f1_macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)
        results['kappa'] = cohen_kappa_score(y_true, y_pred)
        
        # Per-class metrics
        for i, class_name in enumerate(self.class_names):
            results[f'precision_{class_name}'] = precision_score(y_true, y_pred, labels=[i], average=None)[0]
            results[f'recall_{class_name}'] = recall_score(y_true, y_pred, labels=[i], average=None)[0]
            results[f'f1_{class_name}'] = f1_score(y_true, y_pred, labels=[i], average=None)[0]
        
        # ROC AUC if probabilities available
        if y_proba is not None and y_proba.shape[1] == len(self.class_names):
            try:
                results['roc_auc_ovo'] = roc_auc_score(y_true, y_proba, multi_class='ovo', average='macro')
                results['roc_auc_ovr'] = roc_auc_score(y_true, y_proba, multi_class='ovr', average='macro')
            except:
                results['roc_auc_ovo'] = results['roc_auc_ovr'] = np.nan
        
        # Confusion matrix
        cm = confusion_matrix(y_true, y_pred)
        results['confusion_matrix'] = cm.tolist()
        
        # Classification report
        report = classification_report(y_true, y_pred, target_names=self.class_names, output_dict=True)
        results['classification_report'] = report
        
        self.results[model_name] = results
        return results
    
    def print_detailed_report(self, y_true: np.ndarray, y_pred: np.ndarray, 
                             model_name: str = "Model"):
        """Print comprehensive evaluation report"""
        print("=" * 70)
        print(f"EVALUATION REPORT: {model_name}")
        print("=" * 70)
        
        # Calculate metrics
        metrics = self.comprehensive_evaluation(y_true, y_pred, model_name=model_name)
        
        # Print overall metrics
        print(f"\nOverall Metrics:")
        print(f"  Accuracy:  {metrics['accuracy']:.4f}")
        print(f"  F1-Macro:  {metrics['f1_macro']:.4f}")
        print(f"  Precision: {metrics['precision_macro']:.4f}")
        print(f"  Recall:    {metrics['recall_macro']:.4f}")
        print(f"  Kappa:     {metrics['kappa']:.4f}")
        
        # Print per-class metrics
        print(f"\nPer-Class Metrics:")
        for i, class_name in enumerate(self.class_names):
            print(f"  {class_name:12s}: "
                  f"Precision={metrics[f'precision_{class_name}']:.3f}, "
                  f"Recall={metrics[f'recall_{class_name}']:.3f}, "
                  f"F1={metrics[f'f1_{class_name}']:.3f}")
        
        # Print confusion matrix
        print(f"\nConfusion Matrix:")
        cm = np.array(metrics['confusion_matrix'])
        cm_df = pd.DataFrame(cm, index=self.class_names, columns=self.class_names)
        print(cm_df.to_string())
        
        print("=" * 70)
        
        return metrics
    
    def plot_confusion_matrix(self, y_true: np.ndarray, y_pred: np.ndarray, 
                             normalize: bool = True, title: str = "Confusion Matrix"):
        """Plot confusion matrix"""
        cm = confusion_matrix(y_true, y_pred)
        
        if normalize:
            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='.2f' if normalize else 'd',
                   cmap='Blues', xticklabels=self.class_names, 
                   yticklabels=self.class_names)
        plt.title(title)
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.tight_layout()
        plt.show()
        
        return cm
    
    def statistical_significance_test(self, y_true: np.ndarray, 
                                     predictions_list: List[np.ndarray],
                                     model_names: List[str]) -> pd.DataFrame:
        """Perform McNemar's test for model comparison"""
        from mlxtend.evaluate import mcnemar_table, mcnemar
        
        results = []
        
        for i in range(len(model_names)):
            for j in range(i + 1, len(model_names)):
                # Create contingency table
                table = mcnemar_table(y_true, 
                                     predictions_list[i], 
                                     predictions_list[j])
                
                # Calculate McNemar's test
                chi2, p = mcnemar(table, exact=False, corrected=True)
                
                results.append({
                    'Model_A': model_names[i],
                    'Model_B': model_names[j],
                    'Chi2': chi2,
                    'p_value': p,
                    'significant': p < 0.05
                })
        
        return pd.DataFrame(results)
    
    def save_results(self, filename: str = "evaluation_results.json"):
        """Save evaluation results to JSON file"""
        with open(filename, 'w') as f:
            json.dump(self.results, f, indent=4, default=str)
        print(f"Results saved to {filename}")